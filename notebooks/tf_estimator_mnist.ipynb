{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_estimator_mnist.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kylehounslow/gdg_workshop/blob/master/notebooks/tf_estimator_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "QYyfSRWWIGAl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train a `tf.Estimator` on MNIST dataset  \n",
        "In this notebook we introduce the `tf.Estimator` API  by training a small CNN to make predictions on the MNIST dataset \n",
        "\n",
        "Adopted from here: https://www.tensorflow.org/tutorials/estimators/cnn"
      ]
    },
    {
      "metadata": {
        "id": "3IxQ3zie_MVi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Confirm our runtime has a GPU attached"
      ]
    },
    {
      "metadata": {
        "id": "nTMDlTbt_Rxk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NGYbkAxB_Y2E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import necessary libraries"
      ]
    },
    {
      "metadata": {
        "id": "Pq6dCWDtFxjY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from matplotlib import pyplot as plt\n",
        "plt.style.use('default')\n",
        "%matplotlib inline\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w1-soy9SHTSJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Let's load the dataset and do a quick Exploratory Data Analysis (EDA)!"
      ]
    },
    {
      "metadata": {
        "id": "FFT8nlf7KXBh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Quick description of MNIST:   \n",
        "MNIST is a database of 70,000 handwritten digits (0-9) consisting of 28x28 black and white images and their corresponding class labels.  \n",
        "It is often used as a \"Hello World!\" to Convolutional Neural Networks (CNNs) since it is widely accesible and has a low memory footprint despite its relatively large sample size.  \n",
        "http://yann.lecun.com/exdb/mnist/"
      ]
    },
    {
      "metadata": {
        "id": "esr74Pre1rrV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# use keras `mnist` module to download MNIST dataset and load into train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l5GehulpJVub",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize some of the images in a grid.  \n",
        "See many examples of plotting in Colab [here](https://colab.research.google.com/notebooks/charts.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "vFWFtppMJDKc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18,12))\n",
        "for idx, img in enumerate(X_train[:16]):\n",
        "    plt.subplot(4, 4, idx + 1)\n",
        "    plt.imshow(img, cmap='gray', interpolation='none')\n",
        "    plt.title(\"Class {}\".format(y_train[idx]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x6nRvl4Z1sjY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Observe the class distributions in the train and test set"
      ]
    },
    {
      "metadata": {
        "id": "ngyqaBryO3RH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# convert to pandas dataframe to leverage its `value_counts` and `sort_index` functions in plotting\n",
        "df_train = pd.DataFrame(y_train, columns=['label'])\n",
        "df_test = pd.DataFrame(y_test, columns=['label'])\n",
        "df_train.label.value_counts().sort_index().plot(kind='bar', title='Training set distribution', figsize=(10, 6))\n",
        "plt.show()\n",
        "df_test.label.value_counts().sort_index().plot(kind='bar', title='Test set distribution', figsize=(10, 6))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XENrPhmNaDH-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize class distributions on 2D grid using Principal Component Analysis (PCA) dimensionality reduction"
      ]
    },
    {
      "metadata": {
        "id": "-oSaex00Ucsw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# flatten samples from 2-d (28, 28) to 1-d (784,)\n",
        "n_samples, height, width = X_train.shape\n",
        "X_train_flattened = X_train.reshape((n_samples, height*width))\n",
        "X_train_flattened.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yNnwDHI3SOQv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(X_train_flattened)\n",
        "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ArbyFftwVEdG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18, 12))\n",
        "plt.scatter(x=pca_result[:, 0], \n",
        "            y=pca_result[:, 1], \n",
        "            c=y_train[:], \n",
        "            edgecolor='none', \n",
        "            alpha=0.5,\n",
        "            cmap=plt.get_cmap('jet', 10), \n",
        "            s=30)\n",
        "plt.xlabel('PCA-1')\n",
        "plt.ylabel('PCA-2')\n",
        "plt.colorbar()\n",
        "plt.title(\"MNIST Dataset reduced to two dimensions\")\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gge6-Dii_8by",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define our model function which describes the CNN architecture, loss function, optimizer, etc.  \n",
        "Our network highly resembles the architecture shown below except for a few minor differences in the number of filters, kernel sizes, pooling layers, etc.\n",
        "![image](https://codetolight.files.wordpress.com/2017/11/network.png?w=1108)  \n",
        "Image source: https://codetolight.wordpress.com/2017/11/29/getting-started-with-pytorch-for-deep-learning-part-3-neural-network-basics/  \n",
        "|  \n",
        "|  \n",
        "### Our CNN architecture is as follows (adapted from [here](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py)):  \n",
        "```python\n",
        "  # Input Layer\n",
        "  input_layer = tf.reshape(tensor=features[\"x\"], \n",
        "                           shape=[-1, 28, 28, 1],\n",
        "                          name='input')\n",
        "\n",
        "  # Convolutional Layer #1\n",
        "  conv1 = tf.layers.conv2d(\n",
        "      inputs=input_layer,\n",
        "      filters=32,\n",
        "      kernel_size=[3, 3],\n",
        "      padding=\"same\",\n",
        "      activation=tf.nn.relu,\n",
        "      name='conv_1')\n",
        "  # Convolutional Layer #2\n",
        "  conv2 = tf.layers.conv2d(\n",
        "      inputs=conv1,\n",
        "      filters=64,\n",
        "      kernel_size=[3, 3],\n",
        "      padding=\"same\",\n",
        "      activation=tf.nn.relu,\n",
        "      name='conv_2')\n",
        "  pool1 = tf.layers.max_pooling2d(inputs=conv2, \n",
        "                                  pool_size=[2, 2], \n",
        "                                  strides=2,\n",
        "                                 name='pool_1')\n",
        "  dropout1 = tf.layers.dropout(inputs=pool1, \n",
        "                              rate=0.25, \n",
        "                              training=mode == tf.estimator.ModeKeys.TRAIN,\n",
        "                              name='dropout_1')\n",
        "  flatten = tf.layers.Flatten()(dropout1)\n",
        "  dense = tf.layers.dense(inputs=flatten, \n",
        "                          units=128, \n",
        "                          activation=tf.nn.relu,\n",
        "                         name='dense_1')\n",
        "  dropout2 = tf.layers.dropout(inputs=dense, \n",
        "                            rate=0.5, \n",
        "                            training=mode == tf.estimator.ModeKeys.TRAIN,\n",
        "                            name='dropout_2')\n",
        "  logits = tf.layers.dense(inputs=dropout2, \n",
        "                           units=10,\n",
        "                          name='logits')\n",
        "  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "QTJkWw-5chrG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Some notes about writing model functions for `tf.Estimator` (see official docs [here](https://www.tensorflow.org/guide/custom_estimator)):   \n",
        "A model function must have the following signature:  \n",
        "```python\n",
        "def my_model_fn(\n",
        "   features, # This is batch_features from input_fn\n",
        "   labels,   # This is batch_labels from input_fn\n",
        "   mode,     # An instance of tf.estimator.ModeKeys\n",
        "   params):  # Additional configuration\n",
        "```  \n",
        "But let's say we'd like to pass additional parameters to the model function, for example a configurable learning rate or input layer dimensions:  \n",
        "```python\n",
        "def my_model_fn(features, labels, mode, learning_rate, input_height, input_width):\n",
        "    # neural network defined here...\n",
        "```\n",
        "If we pass above model function to `tf.Estimator` as-is, it will throw an `Exception` complaining about unexpected arguments. But we can simply override as follows: \n",
        "```python\n",
        "input_height = 480\n",
        "input_width = 640\n",
        "learning_rate = 0.001\n",
        "def model_fn(features, labels, mode):\n",
        "     return my_model_fn(features, labels, mode, learning_rate, input_height, input_width)\n",
        "```\n",
        "Then instantiate the `tf.Estimator` as per usual:\n",
        "```python\n",
        "estimator = tf.estimator.Estimator(model_fn=model_fn, \n",
        "                                   model_dir=model_dir,\n",
        "                                   ...,\n",
        "                                  )\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "IGVwlE6Y5aYA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cnn_model_fn(features, labels, mode):\n",
        "  \"\"\"Model function for CNN.\"\"\"\n",
        "  # Input Layer\n",
        "  input_layer = tf.reshape(tensor=features[\"x\"], \n",
        "                           shape=[-1, 28, 28, 1],\n",
        "                          name='input')\n",
        "  # Convolutional Layer #1\n",
        "  conv1 = tf.layers.conv2d(\n",
        "      inputs=input_layer,\n",
        "      filters=32,\n",
        "      kernel_size=[3, 3],\n",
        "      padding=\"same\",\n",
        "      activation=tf.nn.relu,\n",
        "      name='conv_1')\n",
        "  # Convolutional Layer #2\n",
        "  conv2 = tf.layers.conv2d(\n",
        "      inputs=conv1,\n",
        "      filters=64,\n",
        "      kernel_size=[3, 3],\n",
        "      padding=\"same\",\n",
        "      activation=tf.nn.relu,\n",
        "      name='conv_2')\n",
        "  pool1 = tf.layers.max_pooling2d(inputs=conv2, \n",
        "                                  pool_size=[2, 2], \n",
        "                                  strides=2,\n",
        "                                 name='pool_1')\n",
        "  dropout1 = tf.layers.dropout(inputs=pool1, \n",
        "                              rate=0.25, \n",
        "                              training=mode == tf.estimator.ModeKeys.TRAIN,\n",
        "                              name='dropout_1')\n",
        "  flatten = tf.layers.Flatten()(dropout1)\n",
        "  dense = tf.layers.dense(inputs=flatten, \n",
        "                          units=128, \n",
        "                          activation=tf.nn.relu,\n",
        "                         name='dense_1')\n",
        "  dropout2 = tf.layers.dropout(inputs=dense, \n",
        "                            rate=0.5, \n",
        "                            training=mode == tf.estimator.ModeKeys.TRAIN,\n",
        "                            name='dropout_2')\n",
        "  logits = tf.layers.dense(inputs=dropout2, \n",
        "                           units=10,\n",
        "                          name='logits')\n",
        "  embeddings = tf.layers.Flatten(name='embeddings')(dense)\n",
        "\n",
        "  predictions = {\n",
        "      # Generate predictions (for PREDICT and EVAL mode)\n",
        "      \"classes\": tf.argmax(input=logits, axis=1),\n",
        "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
        "      # `logging_hook`.\n",
        "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\"),\n",
        "      \"embeddings\": embeddings\n",
        "  }\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
        "\n",
        "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
        "  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "  # add an image summary to visualize input images\n",
        "  tf.summary.image('vis_input', input_layer, max_outputs=4)\n",
        "\n",
        "  # Configure the Training Op (for TRAIN mode)\n",
        "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "    optimizer = tf.train.AdadeltaOptimizer(learning_rate=1)\n",
        "    train_op = optimizer.minimize(\n",
        "        loss=loss,\n",
        "        global_step=tf.train.get_global_step())\n",
        "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
        "\n",
        "  # Add evaluation metrics (for EVAL mode)\n",
        "  eval_metric_ops = {\n",
        "      \"accuracy\": tf.metrics.accuracy(\n",
        "          labels=labels, predictions=predictions[\"classes\"])}\n",
        "  return tf.estimator.EstimatorSpec(\n",
        "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CFyPqKkEChGL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize the CNN model architecture"
      ]
    },
    {
      "metadata": {
        "id": "mcd6p6To6EMa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from IPython.display import  HTML\n",
        "def strip_consts(graph_def, max_const_size=32):\n",
        "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
        "    strip_def = tf.GraphDef()\n",
        "    for n0 in graph_def.node:\n",
        "        n = strip_def.node.add() \n",
        "        n.MergeFrom(n0)\n",
        "        if n.op == 'Const':\n",
        "            tensor = n.attr['value'].tensor\n",
        "            size = len(tensor.tensor_content)\n",
        "            if size > max_const_size:\n",
        "                tensor.tensor_content = tf.compat.as_bytes(\"<stripped %d bytes>\"%size)\n",
        "    return strip_def\n",
        "  \n",
        "def rename_nodes(graph_def, rename_func):\n",
        "    res_def = tf.GraphDef()\n",
        "    for n0 in graph_def.node:\n",
        "        n = res_def.node.add() \n",
        "        n.MergeFrom(n0)\n",
        "        n.name = rename_func(n.name)\n",
        "        for i, s in enumerate(n.input):\n",
        "            n.input[i] = rename_func(s) if s[0]!='^' else '^'+rename_func(s[1:])\n",
        "    return res_def\n",
        "  \n",
        "def show_graph(graph_def, max_const_size=32):\n",
        "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
        "    if hasattr(graph_def, 'as_graph_def'):\n",
        "        graph_def = graph_def.as_graph_def()\n",
        "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
        "    code = \"\"\"\n",
        "        <script>\n",
        "          function load() {{\n",
        "            document.getElementById(\"{id}\").pbtxt = {data};\n",
        "          }}\n",
        "        </script>\n",
        "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
        "        <div style=\"height:600px\">\n",
        "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
        "        </div>\n",
        "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
        "  \n",
        "    iframe = \"\"\"\n",
        "        <iframe seamless style=\"width:800px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
        "    \"\"\".format(code.replace('\"', '&quot;'))\n",
        "    display(HTML(iframe))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cwU-Ok6HC4R1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below is a bit of a hack however it enables us to render the computational graph in a notebook cell"
      ]
    },
    {
      "metadata": {
        "id": "MDRf4N5to7eT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  with tf.Graph().as_default():\n",
        "    mode = tf.estimator.ModeKeys.TRAIN\n",
        "    features = {'x': X_train[0].reshape(1, 28, 28, 1).astype('float32')}\n",
        "    labels = y_train[0].reshape((-1, 1)).astype('int32')\n",
        "    cnn_model_fn(features, labels, mode)\n",
        "    show_graph(tf.get_default_graph())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3K8g47C5aTSB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Let's train the model!"
      ]
    },
    {
      "metadata": {
        "id": "jokNECixay_F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### First, define our model directory where the weights are saved during training "
      ]
    },
    {
      "metadata": {
        "id": "jsbJcELWaxuM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MODEL_DIR = \"/tmp/mnist_convnet_model\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hoTsRgvAvmaM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Before starting training, launch Tensorboard to visualize training process"
      ]
    },
    {
      "metadata": {
        "id": "NzzeRfoEvt0l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using `ngrok` we can tunnel from the hosted runtime to an external address accessible in our browser"
      ]
    },
    {
      "metadata": {
        "id": "Arhp19zAb5vM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# download and install ngrok\n",
        "!wget -q https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip \n",
        "!unzip -o ngrok-stable-linux-amd64.zip > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BfKJv4GRwdkS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# launch tensorboard in shell\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(MODEL_DIR)\n",
        ")\n",
        "import time; time.sleep(10) # wait briefly for tensorboard to launch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "C_JW8EbcbDag",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dWgEvS3kbLPW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Preprocess our images and labels for input into the neural network during training.\n",
        "Currently our images (`X_train`, `X_test`) are in `uint8` format with values 0 to 255. The neural network expects float values 0.0 to 1.0.  \n",
        "Similarily, labels are `uint8` and neural network expects `uint32`"
      ]
    },
    {
      "metadata": {
        "id": "2NgePQV_iaxA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# convert images to float32 with values 0.0 to 1.0\n",
        "X_train_net = X_train.astype('float32')\n",
        "X_test_net = X_test.astype('float32')\n",
        "X_train_net /= 255.\n",
        "X_test_net /= 255.\n",
        "# convert labels to int32 format\n",
        "y_train_net = y_train.astype('int32')\n",
        "y_test_net = y_test.astype('int32')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wX4teAyKgA3u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define batch size and number of training steps\n",
        "* **`batch_size`** is the number of images to feed into the network per training step. In this case we have chosen 128 but for larger images (eg. 640x480) it is likely that this will not fit in memory and a more conservative batch size of 4 or 8 will be chosen.  \n",
        "* **`num_epochs`** is how many times we'd like the neural network to see the entire training set (in this case 60,000 images) during training. I found experimentally that this model converges after ~15 epochs.\n",
        "* **`num_steps`** is how many total training steps will be executed. This is calculated from our batch size and number of epochs. We calculate this since `tf.Estimator` takes `num_steps` as input rather than `num_epochs`"
      ]
    },
    {
      "metadata": {
        "id": "CSY8p2zCcNFI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_epochs = 15\n",
        "num_steps = np.ceil(num_epochs * X_train_net.shape[0] / batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NjVKBKN9b8dP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Instantiate the `tf.Estimator` and point it to our model function and model directory"
      ]
    },
    {
      "metadata": {
        "id": "xKn4X0VpHF5F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ! rm -rf $MODEL_DIR\n",
        "mnist_classifier = tf.estimator.Estimator(\n",
        "    model_fn=cnn_model_fn, \n",
        "    model_dir=MODEL_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rXsvhKvEgIGp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define input function\n",
        "In this case we'll use a simple pre-made input function `tf.estimator.inputs.numpy_input_fn` since our dataset can easily fit in memory.  \n",
        "But in many cases you'll need to write a custom input function for memory or performance reason (see official docs [here](https://www.tensorflow.org/guide/performance/datasets))"
      ]
    },
    {
      "metadata": {
        "id": "6Bk-C1y6gGXH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    x={\"x\": X_train_net},\n",
        "    y=y_train_net,\n",
        "    batch_size=batch_size,\n",
        "    num_epochs=None,\n",
        "    shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aoFtCmEKgyy-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Start the training loop!  \n",
        "Now that we have defined our model function, input function and training hyperparameters, training can begin.  \n",
        "Open the Tensorboard link to monitor the training progress (loss, etc.)"
      ]
    },
    {
      "metadata": {
        "id": "uqqocjHhg1WV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# continuous_eval(estimator=mnist_classifier, model_dir=MODEL_DIR, input_fn=train_input_fn, train_steps=num_steps, name='eval')\n",
        "mnist_classifier.train(\n",
        "    input_fn=train_input_fn,\n",
        "    steps=num_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WlPpa7EJ6AUp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model accuracy using test samples"
      ]
    },
    {
      "metadata": {
        "id": "UG-5si2i6E61",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    x={\"x\": X_test_net},\n",
        "    y=y_test_net,\n",
        "    num_epochs=1,\n",
        "    shuffle=False)\n",
        "eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
        "print(eval_results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lf0ALPfE7sUn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize erroneous model predictions\n",
        "Predict the labels of our test set and visualize where our model is confused"
      ]
    },
    {
      "metadata": {
        "id": "U8RKQC3q780E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    x={\"x\": X_test_net},\n",
        "    num_epochs=1,\n",
        "    shuffle=False)\n",
        "predictions = mnist_classifier.predict(predict_input_fn)\n",
        "max_predictions_to_show = 20\n",
        "show_cnt = 0\n",
        "for predicted, actual, test_img in zip(predictions, y_test, X_test):\n",
        "  pred_class = predicted['classes']\n",
        "  probs = predicted['probabilities']\n",
        "  if pred_class != actual:\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    plt.imshow(test_img)\n",
        "    plt.title(\"Predicted: {}, Actual: {}\".format(pred_class, actual))\n",
        "    plt.show()\n",
        "    show_cnt += 1\n",
        "    if show_cnt > max_predictions_to_show:\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zFkPGGCmyPBo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualizing embeddings  \n",
        "Recall our CNN architecture:  \n",
        "![image](https://codetolight.files.wordpress.com/2017/11/network.png?w=1108)  \n",
        "\n",
        "The primary function of CNNs is to extract meaningful features from image pixels and compress them into a low dimensional representation (embedding) which can then be classified more effectively.  \n",
        "These features are often referred to as **bottleneck features** (further reading [here](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)) and can be highly effective for tasks such as reverse image search, facial recognition, and image encoding since the image is highly compressed but still highly explained by its feature vector.  \n",
        "For instance you can convert a face database into embeddings and use L2 distance to compute similarity between two faces or us K-Nearest-Neighbours to cluster similar images together in a photo album.\n",
        "### To visualize this, we will run the same PCA as before however instead of computing PCA on the raw image pixels we compute on the features from the final dense layer (\"FC Layer\" in above figure) of our trained model.  \n"
      ]
    },
    {
      "metadata": {
        "id": "3jCIM-ILuU8Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = mnist_classifier.predict(predict_input_fn)\n",
        "embeddings = []\n",
        "for predicted in predictions:\n",
        "  embeddings.append(predicted['embeddings'])\n",
        "embeddings = np.array(embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DDhVntYhuRab",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca2 = PCA(n_components=2)\n",
        "pca_result2 = pca2.fit_transform(embeddings)\n",
        "print('Explained variation per principal component: {}'.format(pca2.explained_variance_ratio_))\n",
        "plt.figure(figsize=(18, 12))\n",
        "plt.scatter(x=pca_result2[:, 0], \n",
        "            y=pca_result2[:, 1], \n",
        "            c=y_test[:], \n",
        "            edgecolor='none', \n",
        "            alpha=0.5,\n",
        "            cmap=plt.get_cmap('jet', 10), \n",
        "            s=30)\n",
        "plt.xlabel('PCA-1')\n",
        "plt.ylabel('PCA-2')\n",
        "plt.colorbar()\n",
        "plt.title(\"Predicted embeddings reduced to two dimensions\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qWBwxdKu1pif",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Notice the increase in explained variation and the class boundaries are more clearly defined than before.\n",
        "Let's plot the previous PCA results to compare"
      ]
    },
    {
      "metadata": {
        "id": "ok4eWxpTxpvU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18, 12))\n",
        "plt.scatter(x=pca_result[:, 0], \n",
        "            y=pca_result[:, 1], \n",
        "            c=y_train[:], \n",
        "            edgecolor='none', \n",
        "            alpha=0.5,\n",
        "            cmap=plt.get_cmap('jet', 10), \n",
        "            s=30)\n",
        "plt.xlabel('PCA-1')\n",
        "plt.ylabel('PCA-2')\n",
        "plt.colorbar()\n",
        "plt.title(\"Raw MNIST Dataset reduced to two dimensions\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lxpA_7UD2B7S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BONUS: Use the embeddings to train a \"traditional\" ML algorithm such as `RandomForestClassifier`"
      ]
    },
    {
      "metadata": {
        "id": "cO5TdLAK97Em",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embeddings_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    x={\"x\": X_train_net},\n",
        "    num_epochs=1,\n",
        "    shuffle=False)\n",
        "train_embeddings = []\n",
        "for predicted in mnist_classifier.predict(embeddings_input_fn):\n",
        "  train_embeddings.append(predicted['embeddings'])\n",
        "train_embeddings = np.array(train_embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l-WypA1t9gjQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc = RandomForestClassifier()\n",
        "rfc.fit(train_embeddings, y_train_net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I_AtNDn3-Vpv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preds = rfc.predict(embeddings)\n",
        "from sklearn.metrics import accuracy_score\n",
        "acc = accuracy_score(preds, y_test_net)\n",
        "print(acc)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}